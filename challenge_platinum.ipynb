{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is 86AA-243D\n",
      "\n",
      " Directory of d:\\Binar Academy - Data Science\\challenge_platinum\\challenge_platinum_binar_academy\\collabs_result\n",
      "\n",
      "07/03/2023  13:05    <DIR>          .\n",
      "07/03/2023  12:49    <DIR>          ..\n",
      "07/03/2023  12:47            26.265 app.py\n",
      "01/03/2023  14:54            83.478 challenge_platinum.ipynb\n",
      "07/03/2023  12:44             1.731 function_script.py\n",
      "25/02/2023  20:41        61.005.360 sentiment_analysis_model_challenge.h5\n",
      "07/03/2023  11:43        25.879.240 sentiment_analysis_model_CNN_challenge.h5\n",
      "07/03/2023  11:55    <DIR>          templates\n",
      "01/03/2023  09:52           221.297 test_data.csv\n",
      "01/03/2023  09:52         1.989.823 train_data.csv\n",
      "16/02/2023  19:18         2.186.718 train_preprocess.tsv.txt\n",
      "07/03/2023  12:44    <DIR>          __pycache__\n",
      "               8 File(s)     91.393.912 bytes\n",
      "               4 Dir(s)  157.581.459.456 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memang banyak bacot sih , omongan doang gede b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buat yang berkunjung ke bandung , yang ingin m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalau travelling ke bandung , wajib makan bata...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>warung nasi ampera memiliki konsep rumah makan...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>mbak della sangat baik dan ramah , makanna nya...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>suasana nya sangat romantis jika makan malam d...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>masyarakat tidak kecewa jika dipimpin oleh jok...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>mau itu pak ridwan kamil atau pak dedi mulyadi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets    labels\n",
       "0     tempat yang nyaman untuk berkumpul dengan tema...  positive\n",
       "1     memang banyak bacot sih , omongan doang gede b...  negative\n",
       "2     buat yang berkunjung ke bandung , yang ingin m...  positive\n",
       "3     restoran menyajikan makanan khas sunda yang en...  positive\n",
       "4     kalau travelling ke bandung , wajib makan bata...  positive\n",
       "...                                                 ...       ...\n",
       "9895  warung nasi ampera memiliki konsep rumah makan...  positive\n",
       "9896  mbak della sangat baik dan ramah , makanna nya...  positive\n",
       "9897  suasana nya sangat romantis jika makan malam d...  positive\n",
       "9898  masyarakat tidak kecewa jika dipimpin oleh jok...  positive\n",
       "9899  mau itu pak ridwan kamil atau pak dedi mulyadi...  positive\n",
       "\n",
       "[9900 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasi liwet asep stroberi 1 yang terletak di ja...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tolong dipercepat lagi pengiriman nya .</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>handphone saya lenovo , kecemplung di comberan...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ini suki and barbeque versi murah meriah . lum...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mau melihat bandung dari atas ? nah ini tempat...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>mau petik stroberi nya tetapi ternyata sedang ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>yang benar saja masa nomer whatsapp saya di bl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>food court yang memiliki berbagai macam piliha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>gorengan basi dihidangkan , selera murahan</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>negara indonesia terancam bangkrut .</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets    labels\n",
       "0     nasi liwet asep stroberi 1 yang terletak di ja...  positive\n",
       "1               tolong dipercepat lagi pengiriman nya .  negative\n",
       "2     handphone saya lenovo , kecemplung di comberan...  positive\n",
       "3     ini suki and barbeque versi murah meriah . lum...  positive\n",
       "4     mau melihat bandung dari atas ? nah ini tempat...  positive\n",
       "...                                                 ...       ...\n",
       "1095  mau petik stroberi nya tetapi ternyata sedang ...  positive\n",
       "1096  yang benar saja masa nomer whatsapp saya di bl...  negative\n",
       "1097  food court yang memiliki berbagai macam piliha...  positive\n",
       "1098         gorengan basi dihidangkan , selera murahan  negative\n",
       "1099               negara indonesia terancam bangkrut .  negative\n",
       "\n",
       "[1100 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweets    labels\n",
      "0     tempat yang nyaman untuk berkumpul dengan tema...  positive\n",
      "1     memang banyak bacot sih , omongan doang gede b...  negative\n",
      "2     buat yang berkunjung ke bandung , yang ingin m...  positive\n",
      "3     restoran menyajikan makanan khas sunda yang en...  positive\n",
      "4     kalau travelling ke bandung , wajib makan bata...  positive\n",
      "...                                                 ...       ...\n",
      "9895  warung nasi ampera memiliki konsep rumah makan...  positive\n",
      "9896  mbak della sangat baik dan ramah , makanna nya...  positive\n",
      "9897  suasana nya sangat romantis jika makan malam d...  positive\n",
      "9898  masyarakat tidak kecewa jika dipimpin oleh jok...  positive\n",
      "9899  mau itu pak ridwan kamil atau pak dedi mulyadi...  positive\n",
      "\n",
      "[9900 rows x 2 columns]                                                  tweets    labels\n",
      "0     nasi liwet asep stroberi 1 yang terletak di ja...  positive\n",
      "1               tolong dipercepat lagi pengiriman nya .  negative\n",
      "2     handphone saya lenovo , kecemplung di comberan...  positive\n",
      "3     ini suki and barbeque versi murah meriah . lum...  positive\n",
      "4     mau melihat bandung dari atas ? nah ini tempat...  positive\n",
      "...                                                 ...       ...\n",
      "1095  mau petik stroberi nya tetapi ternyata sedang ...  positive\n",
      "1096  yang benar saja masa nomer whatsapp saya di bl...  negative\n",
      "1097  food court yang memiliki berbagai macam piliha...  positive\n",
      "1098         gorengan basi dihidangkan , selera murahan  negative\n",
      "1099               negara indonesia terancam bangkrut .  negative\n",
      "\n",
      "[1100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(text):\n",
    "    # Make sentence being lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove user, rt, \\n, retweet, \\t, url, xd\n",
    "    pattern_1 = r'(user|retweet|\\\\t|\\\\r|url|xd)'\n",
    "    text = re.sub(pattern_1, '', text)\n",
    "\n",
    "    # Remove mention\n",
    "    pattern_2 = r'@[^\\s]+'\n",
    "    text = re.sub(pattern_2, '', text)\n",
    "\n",
    "    # Remove hashtag\n",
    "    pattern_3 = r'#([^\\s]+)'\n",
    "    text = re.sub(pattern_3, '', text)\n",
    "\n",
    "    # Remove general punctuation, math operation char, etc.\n",
    "    pattern_4 = r'[\\,\\@\\*\\_\\-\\!\\:\\;\\?\\'\\.\\\"\\)\\(\\{\\}\\<\\>\\+\\%\\$\\^\\#\\/\\`\\~\\|\\&\\|]'\n",
    "    text = re.sub(pattern_4, ' ', text)\n",
    "\n",
    "    # Remove single character\n",
    "    pattern_5 = r'\\b\\w{1,3}\\b'\n",
    "    text = re.sub(pattern_5, '', text)\n",
    "\n",
    "    # Remove emoji\n",
    "    pattern_6 = r'\\\\[a-z0-9]{1,5}'\n",
    "    text = re.sub(pattern_6, '', text)\n",
    "\n",
    "    # Remove digit character\n",
    "    pattern_7 = r'\\d+'\n",
    "    text = re.sub(pattern_7, '', text)\n",
    "\n",
    "    # Remove url start with http or https\n",
    "    pattern_8 = r'(https|https:)'\n",
    "    text = re.sub(pattern_8, '', text)\n",
    "\n",
    "    # Remove (\\); ([); (])\n",
    "    pattern_9 = r'[\\\\\\]\\[]'\n",
    "    text = re.sub(pattern_9, '', text)\n",
    "\n",
    "    # Remove character non ASCII\n",
    "    pattern_10 = r'[^\\x00-\\x7f]'\n",
    "    text = re.sub(pattern_10, '', text)\n",
    "\n",
    "    # Remove character non ASCII\n",
    "    pattern_11 = r'(\\\\u[0-9A-Fa-f]+)'\n",
    "    text = re.sub(pattern_11, '', text)\n",
    "\n",
    "    # Remove multiple whitespace\n",
    "    pattern_12 = r'(\\s+|\\\\n)'\n",
    "    text = re.sub(pattern_12, ' ', text)\n",
    "    \n",
    "    # Remove whitespace at the first and end sentences\n",
    "    text = text.rstrip()\n",
    "    text = text.lstrip()\n",
    "    return text\n",
    "\n",
    "def tokenisasi(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean_tweets'] = df_train['tweets'].apply(cleansing)\n",
    "df_test['clean_tweets'] = df_test['tweets'].apply(cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_tokens'] = df_train['clean_tweets'].apply(tokenisasi)\n",
    "df_test['word_tokens'] = df_test['clean_tweets'].apply(tokenisasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memang banyak bacot omongan doang gede bocah k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buat yang berkunjung bandung yang ingin mencob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalau travelling bandung wajib makan batagor r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mengurus kampung menata kota demiz demul ahli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>menyajikan aneka macam ikan bakar disajikan de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>terletak jalan cipaganti bandung seberang bens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jokowi adalah satu satunya presiden yang tidak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alhamdulillah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_tweets\n",
       "0  tempat yang nyaman untuk berkumpul dengan tema...\n",
       "1  memang banyak bacot omongan doang gede bocah k...\n",
       "2  buat yang berkunjung bandung yang ingin mencob...\n",
       "3  restoran menyajikan makanan khas sunda yang en...\n",
       "4  kalau travelling bandung wajib makan batagor r...\n",
       "5      mengurus kampung menata kota demiz demul ahli\n",
       "6  menyajikan aneka macam ikan bakar disajikan de...\n",
       "7  terletak jalan cipaganti bandung seberang bens...\n",
       "8  jokowi adalah satu satunya presiden yang tidak...\n",
       "9                                      alhamdulillah"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['clean_tweets']][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>label_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "      <td>positive</td>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "      <td>[tempat, yang, nyaman, untuk, berkumpul, denga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memang banyak bacot sih , omongan doang gede b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>memang banyak bacot omongan doang gede bocah k...</td>\n",
       "      <td>[memang, banyak, bacot, omongan, doang, gede, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buat yang berkunjung ke bandung , yang ingin m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buat yang berkunjung bandung yang ingin mencob...</td>\n",
       "      <td>[buat, yang, berkunjung, bandung, yang, ingin,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "      <td>[restoran, menyajikan, makanan, khas, sunda, y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalau travelling ke bandung , wajib makan bata...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kalau travelling bandung wajib makan batagor r...</td>\n",
       "      <td>[kalau, travelling, bandung, wajib, makan, bat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets    labels  \\\n",
       "0  tempat yang nyaman untuk berkumpul dengan tema...  positive   \n",
       "1  memang banyak bacot sih , omongan doang gede b...  negative   \n",
       "2  buat yang berkunjung ke bandung , yang ingin m...  positive   \n",
       "3  restoran menyajikan makanan khas sunda yang en...  positive   \n",
       "4  kalau travelling ke bandung , wajib makan bata...  positive   \n",
       "\n",
       "                                        clean_tweets  \\\n",
       "0  tempat yang nyaman untuk berkumpul dengan tema...   \n",
       "1  memang banyak bacot omongan doang gede bocah k...   \n",
       "2  buat yang berkunjung bandung yang ingin mencob...   \n",
       "3  restoran menyajikan makanan khas sunda yang en...   \n",
       "4  kalau travelling bandung wajib makan batagor r...   \n",
       "\n",
       "                                         word_tokens  label_encode  \n",
       "0  [tempat, yang, nyaman, untuk, berkumpul, denga...             2  \n",
       "1  [memang, banyak, bacot, omongan, doang, gede, ...             0  \n",
       "2  [buat, yang, berkunjung, bandung, yang, ingin,...             2  \n",
       "3  [restoran, menyajikan, makanan, khas, sunda, y...             2  \n",
       "4  [kalau, travelling, bandung, wajib, makan, bat...             2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets          object\n",
      "labels          object\n",
      "clean_tweets    object\n",
      "word_tokens     object\n",
      "label_encode     int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_train['label_encode'] = le.fit_transform(df_train['labels'])\n",
    "\n",
    "display(df_train.head())\n",
    "\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweets          0\n",
       "labels          0\n",
       "clean_tweets    0\n",
       "word_tokens     0\n",
       "label_encode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan setiap elemen dalam kolom 'word_tokens' menjadi satu string\n",
    "df_train['string_tokens'] = df_train['word_tokens'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>label_encode</th>\n",
       "      <th>string_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "      <td>positive</td>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "      <td>[tempat, yang, nyaman, untuk, berkumpul, denga...</td>\n",
       "      <td>2</td>\n",
       "      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memang banyak bacot sih , omongan doang gede b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>memang banyak bacot omongan doang gede bocah k...</td>\n",
       "      <td>[memang, banyak, bacot, omongan, doang, gede, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>memang banyak bacot omongan doang gede bocah k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buat yang berkunjung ke bandung , yang ingin m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buat yang berkunjung bandung yang ingin mencob...</td>\n",
       "      <td>[buat, yang, berkunjung, bandung, yang, ingin,...</td>\n",
       "      <td>2</td>\n",
       "      <td>buat yang berkunjung bandung yang ingin mencob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "      <td>positive</td>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "      <td>[restoran, menyajikan, makanan, khas, sunda, y...</td>\n",
       "      <td>2</td>\n",
       "      <td>restoran menyajikan makanan khas sunda yang en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kalau travelling ke bandung , wajib makan bata...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kalau travelling bandung wajib makan batagor r...</td>\n",
       "      <td>[kalau, travelling, bandung, wajib, makan, bat...</td>\n",
       "      <td>2</td>\n",
       "      <td>kalau travelling bandung wajib makan batagor r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>warung nasi ampera memiliki konsep rumah makan...</td>\n",
       "      <td>positive</td>\n",
       "      <td>warung nasi ampera memiliki konsep rumah makan...</td>\n",
       "      <td>[warung, nasi, ampera, memiliki, konsep, rumah...</td>\n",
       "      <td>2</td>\n",
       "      <td>warung nasi ampera memiliki konsep rumah makan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>mbak della sangat baik dan ramah , makanna nya...</td>\n",
       "      <td>positive</td>\n",
       "      <td>mbak della sangat baik ramah makanna juga enak...</td>\n",
       "      <td>[mbak, della, sangat, baik, ramah, makanna, ju...</td>\n",
       "      <td>2</td>\n",
       "      <td>mbak della sangat baik ramah makanna juga enak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>suasana nya sangat romantis jika makan malam d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>suasana sangat romantis jika makan malam sini ...</td>\n",
       "      <td>[suasana, sangat, romantis, jika, makan, malam...</td>\n",
       "      <td>2</td>\n",
       "      <td>suasana sangat romantis jika makan malam sini ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>masyarakat tidak kecewa jika dipimpin oleh jok...</td>\n",
       "      <td>positive</td>\n",
       "      <td>masyarakat tidak kecewa jika dipimpin oleh jok...</td>\n",
       "      <td>[masyarakat, tidak, kecewa, jika, dipimpin, ol...</td>\n",
       "      <td>2</td>\n",
       "      <td>masyarakat tidak kecewa jika dipimpin oleh jok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>mau itu pak ridwan kamil atau pak dedi mulyadi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>ridwan kamil atau dedi mulyadi kita dukung kal...</td>\n",
       "      <td>[ridwan, kamil, atau, dedi, mulyadi, kita, duk...</td>\n",
       "      <td>2</td>\n",
       "      <td>ridwan kamil atau dedi mulyadi kita dukung kal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets    labels  \\\n",
       "0     tempat yang nyaman untuk berkumpul dengan tema...  positive   \n",
       "1     memang banyak bacot sih , omongan doang gede b...  negative   \n",
       "2     buat yang berkunjung ke bandung , yang ingin m...  positive   \n",
       "3     restoran menyajikan makanan khas sunda yang en...  positive   \n",
       "4     kalau travelling ke bandung , wajib makan bata...  positive   \n",
       "...                                                 ...       ...   \n",
       "9895  warung nasi ampera memiliki konsep rumah makan...  positive   \n",
       "9896  mbak della sangat baik dan ramah , makanna nya...  positive   \n",
       "9897  suasana nya sangat romantis jika makan malam d...  positive   \n",
       "9898  masyarakat tidak kecewa jika dipimpin oleh jok...  positive   \n",
       "9899  mau itu pak ridwan kamil atau pak dedi mulyadi...  positive   \n",
       "\n",
       "                                           clean_tweets  \\\n",
       "0     tempat yang nyaman untuk berkumpul dengan tema...   \n",
       "1     memang banyak bacot omongan doang gede bocah k...   \n",
       "2     buat yang berkunjung bandung yang ingin mencob...   \n",
       "3     restoran menyajikan makanan khas sunda yang en...   \n",
       "4     kalau travelling bandung wajib makan batagor r...   \n",
       "...                                                 ...   \n",
       "9895  warung nasi ampera memiliki konsep rumah makan...   \n",
       "9896  mbak della sangat baik ramah makanna juga enak...   \n",
       "9897  suasana sangat romantis jika makan malam sini ...   \n",
       "9898  masyarakat tidak kecewa jika dipimpin oleh jok...   \n",
       "9899  ridwan kamil atau dedi mulyadi kita dukung kal...   \n",
       "\n",
       "                                            word_tokens  label_encode  \\\n",
       "0     [tempat, yang, nyaman, untuk, berkumpul, denga...             2   \n",
       "1     [memang, banyak, bacot, omongan, doang, gede, ...             0   \n",
       "2     [buat, yang, berkunjung, bandung, yang, ingin,...             2   \n",
       "3     [restoran, menyajikan, makanan, khas, sunda, y...             2   \n",
       "4     [kalau, travelling, bandung, wajib, makan, bat...             2   \n",
       "...                                                 ...           ...   \n",
       "9895  [warung, nasi, ampera, memiliki, konsep, rumah...             2   \n",
       "9896  [mbak, della, sangat, baik, ramah, makanna, ju...             2   \n",
       "9897  [suasana, sangat, romantis, jika, makan, malam...             2   \n",
       "9898  [masyarakat, tidak, kecewa, jika, dipimpin, ol...             2   \n",
       "9899  [ridwan, kamil, atau, dedi, mulyadi, kita, duk...             2   \n",
       "\n",
       "                                          string_tokens  \n",
       "0     tempat yang nyaman untuk berkumpul dengan tema...  \n",
       "1     memang banyak bacot omongan doang gede bocah k...  \n",
       "2     buat yang berkunjung bandung yang ingin mencob...  \n",
       "3     restoran menyajikan makanan khas sunda yang en...  \n",
       "4     kalau travelling bandung wajib makan batagor r...  \n",
       "...                                                 ...  \n",
       "9895  warung nasi ampera memiliki konsep rumah makan...  \n",
       "9896  mbak della sangat baik ramah makanna juga enak...  \n",
       "9897  suasana sangat romantis jika makan malam sini ...  \n",
       "9898  masyarakat tidak kecewa jika dipimpin oleh jok...  \n",
       "9899  ridwan kamil atau dedi mulyadi kita dukung kal...  \n",
       "\n",
       "[9900 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(columns=[\"tweets\", \"labels\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14930 unique tokens.\n",
      "Shape of Data Tensor :  (9900, 250)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "sentences = df_train_copy['word_tokens'].to_list()\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of Data Tensor : ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Label Tensor :  (9900, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df_train_copy['label_encode']).values\n",
    "print('Shape of Label Tensor : ', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8910, 250) (8910, 3)\n",
      "(990, 250) (990, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.10, random_state=42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "126/126 [==============================] - 158s 1s/step - loss: 0.6494 - accuracy: 0.7255 - val_loss: 0.4731 - val_accuracy: 0.7924\n",
      "Epoch 2/2\n",
      "126/126 [==============================] - 146s 1s/step - loss: 0.3217 - accuracy: 0.8828 - val_loss: 0.3603 - val_accuracy: 0.8620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc3df97dc0>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 5s 156ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(990, 3)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 2 2 2 0 2 0 2 0 2 0 0 0 2 2 2 2 2 2 2 2 1 0 0 2 0 0 2 0 2 2 2 2 2 1\n",
      " 2 1 2 0 0 2 2 0 2 2 2 2 1 0 2 2 0 1 2 0 2 0 2 2 2 1 1 2 2 2 2 2 0 2 1 2 2\n",
      " 0 2 0 1 2 0 2 1 2 2 2 2 0 2 0 1 0 0 2 2 2 0 2 0 2 1 2 0 2 2 1 2 2 2 1 0 2\n",
      " 2 0 2 2 2 0 2 2 2 2 2 2 2 0 2 0 2 2 2 1 2 2 0 2 2 2 0 2 0 2 2 2 0 1 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 2 0 2 2 2 1 2 2 2 0 1 2 2 2 2 2 2 1 1 2 1 0 2 0 2 2 0\n",
      " 2 2 0 2 2 0 2 2 2 1 0 2 1 2 2 2 2 0 2 0 0 2 2 2 1 2 2 2 2 2 2 0 2 2 2 2 2\n",
      " 1 2 0 2 2 0 0 0 2 2 2 0 2 2 1 2 0 2 0 2 2 1 0 2 0 0 2 2 2 0 2 1 1 1 2 2 2\n",
      " 2 2 1 2 0 1 2 0 2 0 1 2 2 2 2 0 2 0 1 1 2 0 2 2 2 2 2 0 0 2 2 1 2 0 0 2 2\n",
      " 2 2 0 2 2 2 2 0 2 0 0 2 1 2 0 2 2 0 2 1 0 2 2 2 0 1 2 2 0 0 0 2 0 2 2 2 0\n",
      " 0 2 0 0 2 2 2 0 2 2 0 0 0 0 2 2 0 0 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 0 0 2\n",
      " 2 0 2 2 2 0 2 0 1 2 2 0 1 2 2 0 2 2 2 0 2 2 0 2 2 0 2 1 2 1 0 2 2 2 2 2 0\n",
      " 1 2 0 0 0 2 2 2 2 0 0 2 0 2 2 2 2 1 2 0 2 0 2 2 2 2 2 0 2 0 1 1 1 2 2 0 2\n",
      " 0 2 1 2 0 0 0 0 2 0 2 2 2 1 0 0 1 0 0 1 0 0 0 2 2 2 2 2 0 0 2 2 0 2 2 2 0\n",
      " 2 2 2 0 0 0 2 2 2 2 0 0 2 0 0 0 1 2 0 2 2 0 0 1 1 2 0 0 2 0 2 0 2 1 0 0 1\n",
      " 0 2 2 2 2 0 2 2 0 1 2 0 0 2 2 1 2 2 0 0 2 2 2 2 0 0 2 2 2 2 2 2 2 0 0 0 2\n",
      " 2 2 0 2 0 2 2 2 2 2 2 0 0 2 2 2 0 2 1 2 2 0 0 0 2 2 2 2 0 2 2 2 0 1 1 1 0\n",
      " 2 2 2 1 2 2 2 1 2 0 0 2 1 2 0 2 2 0 2 0 0 1 0 2 2 2 2 2 2 1 0 2 1 2 0 2 1\n",
      " 0 2 2 0 1 2 2 2 1 2 2 1 2 2 2 0 2 1 0 0 0 0 2 2 1 1 2 0 2 2 2 2 2 2 2 2 2\n",
      " 2 2 1 1 0 0 2 2 2 0 2 2 1 2 2 0 2 2 2 0 2 1 2 2 0 2 0 2 0 2 2 2 1 2 2 0 2\n",
      " 2 0 1 2 0 2 1 0 2 2 0 2 0 0 1 0 2 2 2 0 2 0 2 0 2 2 2 1 2 0 2 2 0 2 2 2 0\n",
      " 0 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 0 1 2 2 1 2 0 2 0 2 0 0 2 2 2 0 0 0 1 2\n",
      " 0 1 0 1 0 2 2 2 2 2 2 2 2 2 2 0 2 0 2 0 2 2 2 2 2 0 1 2 0 2 2 2 2 2 0 2 0\n",
      " 2 2 2 0 2 0 2 2 0 0 2 2 0 0 1 2 2 2 2 2 2 2 2 0 2 0 2 1 2 2 2 0 1 2 2 2 0\n",
      " 2 2 0 2 0 2 0 2 2 2 0 0 0 0 2 2 0 2 2 2 0 2 0 1 0 0 2 0 2 2 2 0 1 0 0 2 2\n",
      " 0 0 0 2 1 2 2 1 2 0 2 2 0 2 2 0 0 2 1 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 0 2\n",
      " 2 0 1 0 0 2 2 2 0 0 0 2 1 1 0 1 2 0 0 2 2 2 0 2 0 0 2 2 2 2 2 2 2 0 2 1 0\n",
      " 0 2 2 0 0 2 2 2 2 1 2 0 0 2 2 2 0 2 1 2 2 0 2 0 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "print(y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "accuracy = accuracy_score(Y_test_classes, y_pred_classes)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       303\n",
      "           1       0.74      0.79      0.77       101\n",
      "           2       0.92      0.92      0.92       586\n",
      "\n",
      "    accuracy                           0.88       990\n",
      "   macro avg       0.84      0.85      0.84       990\n",
      "weighted avg       0.88      0.88      0.88       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "print(classification_report(Y_test_classes, y_pred_classes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentiment_analysis_model_challenge.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cara menggunakan model kembali dengan load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 5s 120ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "\n",
    "# load data baru\n",
    "df_new = pd.read_csv(\"test_data.csv\")\n",
    "df_new = df_new.applymap(cleansing)\n",
    "# df_new = df_new.applymap(remove_stopwords)\n",
    "\n",
    "# drop kolom label\n",
    "df_new.drop(columns=['labels'], inplace=True)\n",
    "\n",
    "sentences = df_new['tweets'].to_list()\n",
    "\n",
    "# lakukan preprocessing pada data baru\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "X_new = tokenizer.texts_to_sequences(sentences)\n",
    "X_new = pad_sequences(X_new, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "loaded_model = load_model(\"sentiment_analysis_model_challenge.h5\")\n",
    "\n",
    "# lakukan prediksi pada data baru\n",
    "y_prob = loaded_model.predict(X_new)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "# konversi nilai prediksi menjadi label sentimen\n",
    "labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "df_new[\"label_sentimen\"] = [labels[pred] for pred in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label_sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasi liwet asep stroberi yang terletak jalan n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tolong dipercepat lagi pengiriman</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>handphone saya lenovo kecemplung comberan ming...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suki barbeque versi murah meriah lumayan bange...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>melihat bandung dari atas tempat walaupun harg...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>petik stroberi tetapi ternyata sedang tidak mu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>yang benar saja masa nomer whatsapp saya blok ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>food court yang memiliki berbagai macam piliha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>gorengan basi dihidangkan selera murahan</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>negara indonesia terancam bangkrut</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets label_sentimen\n",
       "0     nasi liwet asep stroberi yang terletak jalan n...       positive\n",
       "1                     tolong dipercepat lagi pengiriman       negative\n",
       "2     handphone saya lenovo kecemplung comberan ming...       positive\n",
       "3     suki barbeque versi murah meriah lumayan bange...       positive\n",
       "4     melihat bandung dari atas tempat walaupun harg...       positive\n",
       "...                                                 ...            ...\n",
       "1095  petik stroberi tetapi ternyata sedang tidak mu...       positive\n",
       "1096  yang benar saja masa nomer whatsapp saya blok ...       positive\n",
       "1097  food court yang memiliki berbagai macam piliha...       positive\n",
       "1098           gorengan basi dihidangkan selera murahan       negative\n",
       "1099                 negara indonesia terancam bangkrut       negative\n",
       "\n",
       "[1100 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cara menggunakan kembali model yang telah dibuat (load model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 53s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "\n",
    "# load data baru\n",
    "df_new = pd.read_csv(\"data.csv\", encoding='latin-1')\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(cleansing)\n",
    "# df_new['Tweet'] = df_new['Tweet'].apply(remove_stopwords)\n",
    "\n",
    "sentences = df_new['Tweet'].to_list()\n",
    "\n",
    "# lakukan preprocessing pada data baru\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "X_new = tokenizer.texts_to_sequences(sentences)\n",
    "X_new = pad_sequences(X_new, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "loaded_model = load_model(\"D:\\Binar Academy - Data Science\\challenge_platinum\\challenge_platinum_binar_academy\\sentiment_analysis_model_challenge.h5\")\n",
    "\n",
    "# lakukan prediksi pada data baru\n",
    "y_prob = loaded_model.predict(X_new)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "# konversi nilai prediksi menjadi label sentimen\n",
    "labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "df_new[\"label_sentimen\"] = [labels[pred] for pred in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disaat semua cowok berusaha melacak perhatian ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siapa yang telat ngasih edan sarap bergaul den...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang berfikir kenapa tetap percaya pada tuha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>matamu sipit tapi diliat dari mana</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kapir udah keliatan dongoknya dari...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet     Label\n",
       "0  disaat semua cowok berusaha melacak perhatian ...  positive\n",
       "1  siapa yang telat ngasih edan sarap bergaul den...  positive\n",
       "2  kadang berfikir kenapa tetap percaya pada tuha...  positive\n",
       "3                 matamu sipit tapi diliat dari mana  negative\n",
       "4  kaum cebong kapir udah keliatan dongoknya dari...  negative"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_predict = pd.DataFrame({\"Tweet\": df_new['Tweet'],\n",
    "                           \"Label\": df_new['label_sentimen']\n",
    "                        })\n",
    "df_tweet_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disaat semua cowok berusaha melacak perhatian ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siapa yang telat ngasih edan sarap bergaul den...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang berfikir kenapa tetap percaya pada tuha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>matamu sipit tapi diliat dari mana</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kapir udah keliatan dongoknya dari...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13164</th>\n",
       "      <td>jangan asal ngomong ndasmu congor sekate anjyng</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13165</th>\n",
       "      <td>kasur mana enak kunyuk</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13166</th>\n",
       "      <td>hati hati bisu bosan huft</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13167</th>\n",
       "      <td>yang real mudah terdeteksi yang terkubur suatu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13168</th>\n",
       "      <td>mana situ ngasih cuma foto kutil onta</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet     Label\n",
       "0      disaat semua cowok berusaha melacak perhatian ...  positive\n",
       "1      siapa yang telat ngasih edan sarap bergaul den...  positive\n",
       "2      kadang berfikir kenapa tetap percaya pada tuha...  positive\n",
       "3                     matamu sipit tapi diliat dari mana  negative\n",
       "4      kaum cebong kapir udah keliatan dongoknya dari...  negative\n",
       "...                                                  ...       ...\n",
       "13164    jangan asal ngomong ndasmu congor sekate anjyng  negative\n",
       "13165                             kasur mana enak kunyuk  negative\n",
       "13166                          hati hati bisu bosan huft  negative\n",
       "13167  yang real mudah terdeteksi yang terkubur suatu...  negative\n",
       "13168              mana situ ngasih cuma foto kutil onta  positive\n",
       "\n",
       "[13169 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_predict['Label'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "248/248 [==============================] - 32s 122ms/step - loss: 0.6087 - accuracy: 0.7372 - val_loss: 0.4606 - val_accuracy: 0.8106\n",
      "Epoch 2/2\n",
      "248/248 [==============================] - 32s 131ms/step - loss: 0.3335 - accuracy: 0.8784 - val_loss: 0.3973 - val_accuracy: 0.8566\n",
      "62/62 - 3s - loss: 0.3973 - accuracy: 0.8566 - 3s/epoch - 49ms/step\n",
      "Test accuracy: 0.8565656542778015\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "#data = pd.read_csv(\"train_preprocess.tsv.txt\", sep=\"\\t\", engine=\"python\", names=[\"data\", \"label\"])\n",
    "sentences = df_train['word_tokens'].to_list()\n",
    "labels = df_train['label_encode']\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to numeric values\n",
    "#label_dict = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "#labels = np.array([label_dict[label] for label in labels])\n",
    "#labels = np.array([labels])\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=250)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, 3)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, 3)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=250),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentiment_analysis_model_CNN_challenge.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mtrain_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Preprocess the data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m texts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     17\u001b[0m labels \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     19\u001b[0m \u001b[39m# Create a vocabulary of all the unique words in the texts\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# tokenize text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# pad sequences\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# define model\n",
    "model = keras.Sequential([\n",
    "     keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "     keras.layers.Flatten(),\n",
    "     keras.layers.Dense(64, activation='relu'),\n",
    "     keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c28da407b5413b3940d87ecdae5ea8ce0c2929d84f560e9f5daaaa2573d53e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4460,"status":"ok","timestamp":1676774671832,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"DM7WCpTgh1Na","outputId":"d528d4eb-02c9-40f2-ff1f-9ce8304b0dfc"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Error loading indonesian: Package 'indonesian' not found\n","[nltk_data]     in index\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('indonesian')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","nltk.corpus.stopwords.words('indonesian')\n","from collections import Counter\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout, SpatialDropout1D, LSTM\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import  train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import  accuracy_score, confusion_matrix, classification_report\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import LabelEncoder\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":951,"status":"ok","timestamp":1676774704777,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"dfyqOnawi3wx"},"outputs":[],"source":["#df = pd.read_csv(\"train_preprocess.tsv.txt\", sep=\"\\t\", engine=\"python\", names=[\"data\", \"label\"])\n","df = pd.read_csv(\"D:\\github branch bagas\\challenge_platinum_binar_academy\\\\train_data.csv\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":737,"status":"ok","timestamp":1676774707533,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"RNTlwlich1Nz","outputId":"383773f5-5209-4e11-b732-15cad6fc63c3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweets</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>memang banyak bacot sih , omongan doang gede b...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>buat yang berkunjung ke bandung , yang ingin m...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>restoran menyajikan makanan khas sunda yang en...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kalau travelling ke bandung , wajib makan bata...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9895</th>\n","      <td>warung nasi ampera memiliki konsep rumah makan...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9896</th>\n","      <td>mbak della sangat baik dan ramah , makanna nya...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9897</th>\n","      <td>suasana nya sangat romantis jika makan malam d...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9898</th>\n","      <td>masyarakat tidak kecewa jika dipimpin oleh jok...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9899</th>\n","      <td>mau itu pak ridwan kamil atau pak dedi mulyadi...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9900 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                 tweets    labels\n","0     tempat yang nyaman untuk berkumpul dengan tema...  positive\n","1     memang banyak bacot sih , omongan doang gede b...  negative\n","2     buat yang berkunjung ke bandung , yang ingin m...  positive\n","3     restoran menyajikan makanan khas sunda yang en...  positive\n","4     kalau travelling ke bandung , wajib makan bata...  positive\n","...                                                 ...       ...\n","9895  warung nasi ampera memiliki konsep rumah makan...  positive\n","9896  mbak della sangat baik dan ramah , makanna nya...  positive\n","9897  suasana nya sangat romantis jika makan malam d...  positive\n","9898  masyarakat tidak kecewa jika dipimpin oleh jok...  positive\n","9899  mau itu pak ridwan kamil atau pak dedi mulyadi...  positive\n","\n","[9900 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":620,"status":"ok","timestamp":1676774710797,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"5ihOq4anh1N1","outputId":"7fad2c19-01ea-4d99-8bdf-6270a11309ae"},"outputs":[{"data":{"text/plain":["(9900, 2)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1676774712659,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"DhY60IVmh1N3"},"outputs":[],"source":["def cleansing(text):\n","    # Make sentence being lowercase\n","    text = text.lower()\n","\n","    # Remove user, rt, \\n, retweet, \\t, url, xd\n","    pattern_1 = r'(user|retweet|\\\\t|\\\\r|url|xd)'\n","    text = re.sub(pattern_1, '', text)\n","\n","    # Remove mention\n","    pattern_2 = r'@[^\\s]+'\n","    text = re.sub(pattern_2, '', text)\n","\n","    # Remove hashtag\n","    pattern_3 = r'#([^\\s]+)'\n","    text = re.sub(pattern_3, '', text)\n","\n","    # Remove general punctuation, math operation char, etc.\n","    pattern_4 = r'[\\,\\@\\*\\_\\-\\!\\:\\;\\?\\'\\.\\\"\\)\\(\\{\\}\\<\\>\\+\\%\\$\\^\\#\\/\\`\\~\\|\\&\\|]'\n","    text = re.sub(pattern_4, ' ', text)\n","\n","    # Remove single character\n","    pattern_5 = r'\\b\\w{1,3}\\b'\n","    text = re.sub(pattern_5, '', text)\n","\n","    # Remove emoji\n","    pattern_6 = r'\\\\[a-z0-9]{1,5}'\n","    text = re.sub(pattern_6, '', text)\n","\n","    # Remove digit character\n","    pattern_7 = r'\\d+'\n","    text = re.sub(pattern_7, '', text)\n","\n","    # Remove url start with http or https\n","    pattern_8 = r'(https|https:)'\n","    text = re.sub(pattern_8, '', text)\n","\n","    # Remove (\\); ([); (])\n","    pattern_9 = r'[\\\\\\]\\[]'\n","    text = re.sub(pattern_9, '', text)\n","\n","    # Remove character non ASCII\n","    pattern_10 = r'[^\\x00-\\x7f]'\n","    text = re.sub(pattern_10, '', text)\n","\n","    # Remove character non ASCII\n","    pattern_11 = r'(\\\\u[0-9A-Fa-f]+)'\n","    text = re.sub(pattern_11, '', text)\n","\n","    # Remove multiple whitespace\n","    pattern_12 = r'(\\s+|\\\\n)'\n","    text = re.sub(pattern_12, ' ', text)\n","    \n","    # Remove whitespace at the first and end sentences\n","    text = text.rstrip()\n","    text = text.lstrip()\n","    return text\n","\n","indo_stop_words = stopwords.words(\"indonesian\")\n","\n","def remove_stopwords(text):\n","    return ' '.join([word for word in word_tokenize(text) if word not in indo_stop_words])\n","\n","def tokenisasi(text):\n","    tokens = nltk.tokenize.word_tokenize(text)\n","    return tokens\n","\n","def stemming(tokens):\n","    factory = StemmerFactory()\n","    stemmer = factory.create_stemmer()\n","    return ' '.join([stemmer.stem(token) for token in tokens])"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6962,"status":"ok","timestamp":1676774723598,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"9HXne__Nh1N8"},"outputs":[],"source":["df_coba = df.applymap(cleansing)\n","#df_coba = df_coba.applymap(remove_stopwords)\n","#df_coba = df_coba.applymap(stemming)\n","df_coba['word_token'] = df_coba['tweets'].apply(tokenisasi)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":419,"status":"ok","timestamp":1676774726333,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"dZTRuKqBb1U2","outputId":"40ba5189-9984-4f94-dbb6-ea645584d761"},"outputs":[],"source":["# Menggabungkan setiap elemen dalam kolom 'word_tokens' menjadi satu string\n","df_coba['string_token'] = df_coba['word_token'].apply(' '.join)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":467,"status":"ok","timestamp":1676774740373,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"tjWcV310gYm2"},"outputs":[],"source":["# membuat dictionary mapping\n","#mapping = {'positive': 1, 'neutral': 0, 'negative': 2}"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1676774742287,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"oDgG7QJdgd7V"},"outputs":[],"source":["# mengubah nilai pada kolom 'label' dengan method apply\n","#df_coba['label'] = df_coba['label'].apply(lambda x: [mapping[i] for i in x]) --> hanya jika isi kolom 'label' berbentuk list atau array\n","#df_coba['label'] = df_coba['label'].map(mapping)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweets</th>\n","      <th>labels</th>\n","      <th>word_token</th>\n","      <th>string_token</th>\n","      <th>label_encode</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n","      <td>positive</td>\n","      <td>[tempat, yang, nyaman, untuk, berkumpul, denga...</td>\n","      <td>tempat yang nyaman untuk berkumpul dengan tema...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>memang banyak bacot omongan doang gede bocah k...</td>\n","      <td>negative</td>\n","      <td>[memang, banyak, bacot, omongan, doang, gede, ...</td>\n","      <td>memang banyak bacot omongan doang gede bocah k...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>buat yang berkunjung bandung yang ingin mencob...</td>\n","      <td>positive</td>\n","      <td>[buat, yang, berkunjung, bandung, yang, ingin,...</td>\n","      <td>buat yang berkunjung bandung yang ingin mencob...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>restoran menyajikan makanan khas sunda yang en...</td>\n","      <td>positive</td>\n","      <td>[restoran, menyajikan, makanan, khas, sunda, y...</td>\n","      <td>restoran menyajikan makanan khas sunda yang en...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kalau travelling bandung wajib makan batagor r...</td>\n","      <td>positive</td>\n","      <td>[kalau, travelling, bandung, wajib, makan, bat...</td>\n","      <td>kalau travelling bandung wajib makan batagor r...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              tweets    labels  \\\n","0  tempat yang nyaman untuk berkumpul dengan tema...  positive   \n","1  memang banyak bacot omongan doang gede bocah k...  negative   \n","2  buat yang berkunjung bandung yang ingin mencob...  positive   \n","3  restoran menyajikan makanan khas sunda yang en...  positive   \n","4  kalau travelling bandung wajib makan batagor r...  positive   \n","\n","                                          word_token  \\\n","0  [tempat, yang, nyaman, untuk, berkumpul, denga...   \n","1  [memang, banyak, bacot, omongan, doang, gede, ...   \n","2  [buat, yang, berkunjung, bandung, yang, ingin,...   \n","3  [restoran, menyajikan, makanan, khas, sunda, y...   \n","4  [kalau, travelling, bandung, wajib, makan, bat...   \n","\n","                                        string_token  label_encode  \n","0  tempat yang nyaman untuk berkumpul dengan tema...             2  \n","1  memang banyak bacot omongan doang gede bocah k...             0  \n","2  buat yang berkunjung bandung yang ingin mencob...             2  \n","3  restoran menyajikan makanan khas sunda yang en...             2  \n","4  kalau travelling bandung wajib makan batagor r...             2  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["tweets          object\n","labels          object\n","word_token      object\n","string_token    object\n","label_encode     int32\n","dtype: object\n"]}],"source":["le = LabelEncoder()\n","\n","df_coba['label_encode'] = le.fit_transform(df_coba['labels'])\n","\n","display(df_coba.head())\n","\n","print(df_coba.dtypes)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":433,"status":"ok","timestamp":1676774748381,"user":{"displayName":"Davinca Rizqy","userId":"05860212563616590890"},"user_tz":-420},"id":"T83xw7xAgJge","outputId":"28cca3f7-d7d1-431e-e251-d49ed960b241"},"outputs":[{"data":{"text/plain":["tweets          0\n","labels          0\n","word_token      0\n","string_token    0\n","label_encode    0\n","dtype: int64"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_coba.isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Feedforward Neural Network\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'list' object has no attribute 'lower'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[34], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m tokenizer \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Fit TF-IDF Vectorizer to the data\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m tfidf_vectorizer\u001b[39m.\u001b[39;49mfit(sentences)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Get word index and vocabulary size\u001b[39;00m\n\u001b[0;32m     21\u001b[0m word_index \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mvocabulary_\n","File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2101\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2094\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2095\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2096\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2097\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2098\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2099\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2100\u001b[0m )\n\u001b[1;32m-> 2101\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1275\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n","File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n","File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n","\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"]}],"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Load the dataset\n","#df = pd.read_csv(\"tweets.csv\")\n","\n","# Prepare the data\n","sentences = df_coba['word_token'].tolist()\n","labels = df_coba['label_encode'].tolist()\n","\n","# Initialize TF-IDF Vectorizer\n","tokenizer = TfidfVectorizer(stop_words=None)\n","\n","# Fit TF-IDF Vectorizer to the data\n","tfidf_vectorizer.fit(sentences)\n","\n","# Get word index and vocabulary size\n","word_index = tfidf_vectorizer.vocabulary_\n","vocab_size = len(word_index) + 1\n","\n","# Convert text to sequences\n","sequences = tfidf_vectorizer.transform(sentences)\n","\n","# Perform padding on sequences\n","padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences.toarray(), maxlen=250)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, labels, train_size=0.8)\n","\n","# One-hot encode the labels\n","Y_train = tf.keras.utils.to_categorical(Y_train)\n","Y_test = tf.keras.utils.to_categorical(Y_test)\n","\n","# Build the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.3),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(2, activation='softmax')\n","])\n","\n","# Compile the model\n","opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data=(X_test, Y_test))\n","\n","# Evaluate the model on test set\n","test_loss, test_acc = model.evaluate(X_test, Y_test)\n","print('Test accuracy:', test_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["laura"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CNN"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","248/248 [==============================] - 36s 139ms/step - loss: 0.5939 - accuracy: 0.7484 - val_loss: 0.4545 - val_accuracy: 0.7955\n","Epoch 2/2\n","248/248 [==============================] - 36s 146ms/step - loss: 0.3188 - accuracy: 0.8806 - val_loss: 0.3682 - val_accuracy: 0.8687\n","62/62 - 1s - loss: 0.3682 - accuracy: 0.8687 - 1s/epoch - 18ms/step\n","Test accuracy: 0.868686854839325\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load data\n","#data = pd.read_csv(\"train_preprocess.tsv.txt\", sep=\"\\t\", engine=\"python\", names=[\"data\", \"label\"])\n","sentences = df_coba['word_token'].to_list()\n","labels = df_coba['label_encode'].to_list()\n","\n","\n","\n","# Convert labels to numeric values\n","#label_dict = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n","#labels = np.array([label_dict[label] for label in labels])\n","#labels = np.array([labels])\n","\n","# Tokenize data\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=250)\n","\n","# Split data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n","\n","# One-hot encode the labels\n","Y_train = tf.keras.utils.to_categorical(Y_train, 3)\n","Y_test = tf.keras.utils.to_categorical(Y_test, 3)\n","\n","# Define the model architecture\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=250),\n","    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n","    tf.keras.layers.MaxPooling1D(5),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(3, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","batch_size = 32\n","epochs = 2\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\n","\n","# Evaluate the model on test set\n","test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n","print(\"Test accuracy:\", test_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGmGOCLZfgke"},"outputs":[],"source":["# mengubah nilai pada kolom 'label' menjadi integer\n","#df_coba['label'] = df_coba['label'].apply(lambda x: x[0]).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#text = df_coba['word_token'].tolist()\n","#print(len(text))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#token = Tokenizer()\n","#token.fit_on_texts(text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df_coba['word_to_index'] = token.texts_to_sequences(text)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Save CNN Model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["model.save(\"sentiment_analysis_model_CNN_challenge.h5\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import os\n","\n","#current_directory = os.getcwd()\n","#print(\"Current working directory:\", current_directory)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## preprocessing tambahan untuk LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_copy = df_coba.copy()\n","df_copy.drop(columns=[\"data\",\"label\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_token</th>\n","      <th>string_token</th>\n","      <th>label_encode</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[warung, dimiliki, oleh, pengusaha, pabrik, ta...</td>\n","      <td>warung dimiliki oleh pengusaha pabrik tahu yan...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[mohon, ulama, lurus, k, mmbri, hujjah, partai...</td>\n","      <td>mohon ulama lurus k mmbri hujjah partai yang h...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[lokasi, strategis, jalan, sumatera, bandung, ...</td>\n","      <td>lokasi strategis jalan sumatera bandung tempat...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[betapa, bahagia, diri, saat, unboxing, paket,...</td>\n","      <td>betapa bahagia diri saat unboxing paket barang...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[jadi, mahasiswa, jangan, sombong, dong, kasih...</td>\n","      <td>jadi mahasiswa jangan sombong dong kasih kartu...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10995</th>\n","      <td>[tidak, kecewa]</td>\n","      <td>tidak kecewa</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10996</th>\n","      <td>[enak, rasa, masakan, apalagi, kepiting, yang,...</td>\n","      <td>enak rasa masakan apalagi kepiting yang menyen...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10997</th>\n","      <td>[hormati, partai, partai, yang, telah, berkoal...</td>\n","      <td>hormati partai partai yang telah berkoalisi</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10998</th>\n","      <td>[pagi, pagi, pasteur, sudah, macet, parah, bik...</td>\n","      <td>pagi pagi pasteur sudah macet parah bikin jeng...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10999</th>\n","      <td>[meskipun, sering, belanja, yogya, riau, junct...</td>\n","      <td>meskipun sering belanja yogya riau junction ta...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                              word_token  \\\n","0      [warung, dimiliki, oleh, pengusaha, pabrik, ta...   \n","1      [mohon, ulama, lurus, k, mmbri, hujjah, partai...   \n","2      [lokasi, strategis, jalan, sumatera, bandung, ...   \n","3      [betapa, bahagia, diri, saat, unboxing, paket,...   \n","4      [jadi, mahasiswa, jangan, sombong, dong, kasih...   \n","...                                                  ...   \n","10995                                    [tidak, kecewa]   \n","10996  [enak, rasa, masakan, apalagi, kepiting, yang,...   \n","10997  [hormati, partai, partai, yang, telah, berkoal...   \n","10998  [pagi, pagi, pasteur, sudah, macet, parah, bik...   \n","10999  [meskipun, sering, belanja, yogya, riau, junct...   \n","\n","                                            string_token  label_encode  \n","0      warung dimiliki oleh pengusaha pabrik tahu yan...             2  \n","1      mohon ulama lurus k mmbri hujjah partai yang h...             1  \n","2      lokasi strategis jalan sumatera bandung tempat...             2  \n","3      betapa bahagia diri saat unboxing paket barang...             2  \n","4      jadi mahasiswa jangan sombong dong kasih kartu...             0  \n","...                                                  ...           ...  \n","10995                                       tidak kecewa             2  \n","10996  enak rasa masakan apalagi kepiting yang menyen...             2  \n","10997        hormati partai partai yang telah berkoalisi             1  \n","10998  pagi pagi pasteur sudah macet parah bikin jeng...             0  \n","10999  meskipun sering belanja yogya riau junction ta...             2  \n","\n","[11000 rows x 3 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["df_copy"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# NAIVE BAYES"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizer = CountVectorizer()\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(df_copy['string_token'], df_copy['label_encode'], train_size=0.8, random_state=42)\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb = MultinomialNB() # Panggil MODEL NB\n","nb.fit(X_train, Y_train) # Training data\n","Y_pred = nb.predict(X_test) # Prediksi data X_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 528,   69,  116],\n","       [   5,  135,    7],\n","       [ 147,   35, 1158]], dtype=int64)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(Y_pred, Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.74      0.76       713\n","           1       0.56      0.92      0.70       147\n","           2       0.90      0.86      0.88      1340\n","\n","    accuracy                           0.83      2200\n","   macro avg       0.75      0.84      0.78      2200\n","weighted avg       0.84      0.83      0.83      2200\n","\n"]}],"source":["print(classification_report(Y_pred, Y_test))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_copy['word_token'] = df_copy['word_token'].to_list()\n","df_copy['label_encode'] = df_copy['label_encode'].to_list()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 15763 unique tokens.\n","Shape of data tensor: (11000, 250)\n"]}],"source":["MAX_NB_WORDS = 50000\n","MAX_SEQUENCE_LENGTH = 250\n","EMBEDDING_DIM = 100\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+.,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(df_copy['word_token'].values)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","X = tokenizer.texts_to_sequences(df_copy['word_token'].values)\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', X.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of label tensor: (11000, 3)\n"]}],"source":["Y = pd.get_dummies(df_copy['label_encode']).values\n","print('Shape of label tensor:', Y.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(9900, 250) (9900, 3)\n","(1100, 250) (1100, 3)\n"]}],"source":["X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.10, random_state=42)\n","print(X_train.shape, Y_train.shape)\n","print(X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","140/140 [==============================] - 180s 1s/step - loss: 0.6075 - accuracy: 0.7536 - val_loss: 0.4370 - val_accuracy: 0.8192\n","Epoch 2/2\n","140/140 [==============================] - 173s 1s/step - loss: 0.2898 - accuracy: 0.8984 - val_loss: 0.3519 - val_accuracy: 0.8717\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1caf178ca90>"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["model = Sequential()\n","model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(3, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","epochs = 2\n","batch_size = 64\n","\n","model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["35/35 [==============================] - 7s 122ms/step\n"]},{"data":{"text/plain":["(1100, 3)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["Y_pred = model.predict(X_test)\n","Y_pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 1 2 ... 2 2 0]\n"]}],"source":["#from mlxtend.preprocessing import one_hote\n","#result = np.where(Y_pred[0]==np.amax(Y_pred[0]))\n","#one_hot(result[0])\n","\n","Y_pred_labels = np.argmax(Y_pred, axis=1)\n","print(Y_pred_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 88.00%\n"]}],"source":["Y_pred_classes = np.argmax(Y_pred, axis=1)\n","Y_test_classes = np.argmax(Y_test, axis=1)\n","accuracy = accuracy_score(Y_test_classes, Y_pred_classes)\n","print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.81      0.88      0.84       342\n","           1       0.85      0.78      0.81       116\n","           2       0.93      0.90      0.91       642\n","\n","    accuracy                           0.88      1100\n","   macro avg       0.86      0.85      0.86      1100\n","weighted avg       0.88      0.88      0.88      1100\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","Y_pred_classes = np.argmax(Y_pred, axis=1)\n","Y_test_classes = np.argmax(Y_test, axis=1)\n","print(classification_report(Y_test_classes, Y_pred_classes))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Save LSTM Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save(\"sentiment_analysis_model_LSTM_challenge.h5\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Cara Menggunakan model kembali dengan load model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["344/344 [==============================] - 34s 97ms/step\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import load_model\n","\n","# load data baru\n","df_new = pd.read_csv(\"train_preprocess.tsv.txt\", sep=\"\\t\", engine=\"python\", names=[\"data\", \"label\"])\n","df_new = df_new.applymap(cleansing)\n","df_new = df_new.applymap(remove_stopwords)\n","\n","# drop kolom label\n","df_new.drop(columns=['label'], inplace=True)\n","\n","# lakukan preprocessing pada data baru\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df_new['data'].values)\n","X_new = tokenizer.texts_to_sequences(df_new['data'].values)\n","X_new = pad_sequences(X_new, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","loaded_model = load_model(\"D:\\github branch davinca\\challenge_platinum_binar_academy\\sentiment_analysis_model_challenge.h5\")\n","\n","# lakukan prediksi pada data baru\n","Y_prob = loaded_model.predict(X_new)\n","Y_pred = Y_prob.argmax(axis=-1)\n","\n","# konversi nilai prediksi menjadi label sentimen\n","labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n","df_new[\"label_sentimen\"] = [labels[pred] for pred in Y_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data</th>\n","      <th>label_sentimen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>warung dimiliki pengusaha pabrik puluhan terke...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mohon ulama lurus k mmbri hujjah partai diwlh ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>lokasi strategis jalan sumatera bandung nyaman...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>betapa bahagia unboxing paket barang bagus men...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>mahasiswa sombong kasih kartu kuning belajar u...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10995</th>\n","      <td>kecewa</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>10996</th>\n","      <td>enak masakan kepiting menyenangkan memilih kep...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>10997</th>\n","      <td>hormati partai partai berkoalisi</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>10998</th>\n","      <td>pagi pagi pasteur macet parah bikin jengkel</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>10999</th>\n","      <td>belanja yogya riau junction kali lihat foodlif...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    data label_sentimen\n","0      warung dimiliki pengusaha pabrik puluhan terke...       positive\n","1      mohon ulama lurus k mmbri hujjah partai diwlh ...       positive\n","2      lokasi strategis jalan sumatera bandung nyaman...       positive\n","3      betapa bahagia unboxing paket barang bagus men...       positive\n","4      mahasiswa sombong kasih kartu kuning belajar u...       negative\n","...                                                  ...            ...\n","10995                                             kecewa       positive\n","10996  enak masakan kepiting menyenangkan memilih kep...       positive\n","10997                   hormati partai partai berkoalisi       positive\n","10998        pagi pagi pasteur macet parah bikin jengkel       negative\n","10999  belanja yogya riau junction kali lihat foodlif...       positive\n","\n","[11000 rows x 2 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["df_new\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Cara menggunakan kembali model yang telah dibuat (load model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["412/412 [==============================] - 43s 103ms/step\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import load_model\n","\n","# load data baru\n","df_new = pd.read_csv(\"original_data.csv\", encoding='latin-1')\n","df_new['Tweet'] = df_new['Tweet'].apply(cleansing)\n","df_new['Tweet'] = df_new['Tweet'].apply(remove_stopwords)\n","\n","# lakukan preprocessing pada data baru\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(df_new['Tweet'].values)\n","X_new = tokenizer.texts_to_sequences(df_new['Tweet'].values)\n","X_new = pad_sequences(X_new, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","loaded_model = load_model(\"D:\\github branch davinca\\challenge_platinum_binar_academy\\sentiment_analysis_model_challenge.h5\")\n","\n","# lakukan prediksi pada data baru\n","Y_prob = loaded_model.predict(X_new)\n","Y_pred = Y_prob.argmax(axis=-1)\n","\n","# konversi nilai prediksi menjadi label sentimen\n","labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n","df_new[\"label\"] = [labels[pred] for pred in Y_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Tweet</th>\n","      <th>HS</th>\n","      <th>Abusive</th>\n","      <th>HS_Individual</th>\n","      <th>HS_Group</th>\n","      <th>HS_Religion</th>\n","      <th>HS_Race</th>\n","      <th>HS_Physical</th>\n","      <th>HS_Gender</th>\n","      <th>HS_Other</th>\n","      <th>HS_Weak</th>\n","      <th>HS_Moderate</th>\n","      <th>HS_Strong</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>disaat cowok berusaha melacak perhatian lantas...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>telat ngasih edan sarap bergaul cigax jifla ca...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>kadang berfikir percaya tuhan jatuh berkali ka...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>matamu sipit diliat</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>kaum cebong kapir udah keliatan dongoknya dong...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13164</th>\n","      <td>13164</td>\n","      <td>ngomong ndasmu congor sekate anjyng</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>13165</th>\n","      <td>13165</td>\n","      <td>kasur enak kunyuk</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>13166</th>\n","      <td>13166</td>\n","      <td>hati hati bisu bosan huft</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>13167</th>\n","      <td>13167</td>\n","      <td>real mudah terdeteksi terkubur dahsyat ledakan...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>13168</th>\n","      <td>13168</td>\n","      <td>situ ngasih foto kutil onta</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13169 rows × 15 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0                                              Tweet  HS  \\\n","0               0  disaat cowok berusaha melacak perhatian lantas...   1   \n","1               1  telat ngasih edan sarap bergaul cigax jifla ca...   0   \n","2               2  kadang berfikir percaya tuhan jatuh berkali ka...   0   \n","3               3                                matamu sipit diliat   0   \n","4               4  kaum cebong kapir udah keliatan dongoknya dong...   1   \n","...           ...                                                ...  ..   \n","13164       13164                ngomong ndasmu congor sekate anjyng   1   \n","13165       13165                                  kasur enak kunyuk   0   \n","13166       13166                          hati hati bisu bosan huft   0   \n","13167       13167  real mudah terdeteksi terkubur dahsyat ledakan...   0   \n","13168       13168                        situ ngasih foto kutil onta   1   \n","\n","       Abusive  HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  \\\n","0            1              1         0            0        0            0   \n","1            1              0         0            0        0            0   \n","2            0              0         0            0        0            0   \n","3            0              0         0            0        0            0   \n","4            1              0         1            1        0            0   \n","...        ...            ...       ...          ...      ...          ...   \n","13164        1              1         0            0        0            1   \n","13165        1              0         0            0        0            0   \n","13166        0              0         0            0        0            0   \n","13167        0              0         0            0        0            0   \n","13168        1              1         0            0        0            0   \n","\n","       HS_Gender  HS_Other  HS_Weak  HS_Moderate  HS_Strong     label  \n","0              0         1        1            0          0  positive  \n","1              0         0        0            0          0  positive  \n","2              0         0        0            0          0  positive  \n","3              0         0        0            0          0  negative  \n","4              0         0        0            1          0  positive  \n","...          ...       ...      ...          ...        ...       ...  \n","13164          0         0        1            0          0   neutral  \n","13165          0         0        0            0          0   neutral  \n","13166          0         0        0            0          0  positive  \n","13167          0         0        0            0          0  positive  \n","13168          0         1        1            0          0  negative  \n","\n","[13169 rows x 15 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["df_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>disaat cowok berusaha melacak perhatian lantas...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>telat ngasih edan sarap bergaul cigax jifla ca...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kadang berfikir percaya tuhan jatuh berkali ka...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>matamu sipit diliat</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kaum cebong kapir udah keliatan dongoknya dong...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Tweet     Label\n","0  disaat cowok berusaha melacak perhatian lantas...  positive\n","1  telat ngasih edan sarap bergaul cigax jifla ca...  positive\n","2  kadang berfikir percaya tuhan jatuh berkali ka...  positive\n","3                                matamu sipit diliat  negative\n","4  kaum cebong kapir udah keliatan dongoknya dong...  positive"]},"execution_count":38,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["df_tweet_predict = pd.DataFrame({\"Tweet\": df_new['Tweet'],\n","                           \"Label\": df_new['label']\n","                        })\n","df_tweet_predict.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>disaat cowok berusaha melacak perhatian lantas...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>telat ngasih edan sarap bergaul cigax jifla ca...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kadang berfikir percaya tuhan jatuh berkali ka...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>matamu sipit diliat</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kaum cebong kapir udah keliatan dongoknya dong...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13164</th>\n","      <td>ngomong ndasmu congor sekate anjyng</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>13165</th>\n","      <td>kasur enak kunyuk</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>13166</th>\n","      <td>hati hati bisu bosan huft</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>13167</th>\n","      <td>real mudah terdeteksi terkubur dahsyat ledakan...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>13168</th>\n","      <td>situ ngasih foto kutil onta</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13169 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   Tweet     Label\n","0      disaat cowok berusaha melacak perhatian lantas...  positive\n","1      telat ngasih edan sarap bergaul cigax jifla ca...  positive\n","2      kadang berfikir percaya tuhan jatuh berkali ka...  positive\n","3                                    matamu sipit diliat  negative\n","4      kaum cebong kapir udah keliatan dongoknya dong...  positive\n","...                                                  ...       ...\n","13164                ngomong ndasmu congor sekate anjyng   neutral\n","13165                                  kasur enak kunyuk   neutral\n","13166                          hati hati bisu bosan huft  positive\n","13167  real mudah terdeteksi terkubur dahsyat ledakan...  positive\n","13168                        situ ngasih foto kutil onta  negative\n","\n","[13169 rows x 2 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["df_tweet_predict\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array(['positive', 'negative', 'neutral'], dtype=object)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["df_tweet_predict['Label'].unique()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# --batas akhir modelling CNN dan LSTM--"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# kode-kode dibawah ini sudah tidak dipakai"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#x = df_copy.iloc[:,1] # parameter (features)\n","#y = df_copy.iloc[:,0] # targeted"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.7)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#print(x_train.shape)  # Output: (n_data_points, n_features)\n","#print(y_train.shape)  # Output: (n_data_points,)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#x_train_dtype = np.array(x_train).dtype\n","#print(x_train_dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#y_train_dtype = np.array(y_train).dtype\n","#print(y_train_dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#MAX_SEQUENCE_LENGTH = 100\n","\n","# Lakukan padding pada setiap data input\n","#x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#MAX_SEQUENCE_LENGTH = 100\n","\n","# Lakukan padding pada setiap data input\n","#x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#x_train = np.array(x_train, dtype=np.int64)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(50, 25, 10), max_iter=500,\n","              random_state=1, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(50, 25, 10), max_iter=500,\n","              random_state=1, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"],"text/plain":["MLPClassifier(alpha=1e-05, hidden_layer_sizes=(50, 25, 10), max_iter=500,\n","              random_state=1, solver='lbfgs')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, 25, 10), random_state=1, max_iter=500)\n","\n","clf.fit(X_train, Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["#y_pred = clf.predict(x_test)\n","#y_pred # sbetulnya masih termasuk error ini "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0.8145454545454546"]},"execution_count":32,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mnotebook controller is DISPOSED. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["accuracy_score(Y_test, Y_pred)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"}}},"nbformat":4,"nbformat_minor":0}
